%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2013,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2013} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2013}

\begin{document} 

\twocolumn[
\icmltitle{Analysis of Self Implemented Logistic Regression}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2013
% package.
\icmlauthor{Carl Cortright}{carl.cortright@colorado.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\section{Introduction}
Logistic regression is a common machine learning technique used for both binary and multiclass classification problems. In this paper, I demonstrate how logistic regression can be used to identify the topic of a document. Specifically, I use a technique called stochastic gradient decent to build a fast and accurate model for classification. Analysis of the forulas used will show how large beta values are used heavily in predicting a class.

\section{Formulas}

\subsection{The Sigmoid Fuction}
The sigmoid function is used heavily in logistic regression because it sqeezes any real value into a number between 0 and 1 (like a probability) and is easily differentiable. To calculate the probability of a given feature vector, we feed the dot product of beta values and the training examples into the sigmoid function.

\subsection{The Delta}
On each beta update, we change each beta value by some amount delta. We know that delta needs to move beta in the right direction, so we take the actual label and subtract the calculated probability and then we multiply by the number of times that feature appears in the feature vector. After multiplying this value by the step function, we add it to the beta vector, allowing us to shift the beta vector is the right direection based on the training example.

\subsection{Regularization}
In order to ensure that our model generalizes well to unseen data, we need to prevent overfitting. The most common way to prevent overfitting in logistic regression is by using regularization, or penalizing large beta values. The way we do this is by shrinking large beta values. We define a shrinkage value that gets larger based on the iteration. We then take all of the features that we just updated and multiply them by our shrinkage number raised to the gap power (how long it has been since we last updated). This will effectively scale the beta value down if it is very large. 

\section{Questions}

\subsection{What is the learning rate?}
The learning rate determines how quickly our model will converge. In this program, the learning rate is determined in our lambda function step and is set to 0.05.

\subsection{How many passes over the data did we need?}
In this program, we are able to converge to a reasonable answer fairly quickly, using 1 or 2 passes over the data. Heuristically, I would guess the sweetspot is somewhere between 2-5 passes. Any more and we would risk overfitting.

\subsection{What are the best predictors?}
The best predictors of each class, are the highest weighted beta values for that class. Hockey was denoted by positive beta values and when we sorted them we found that the highest predictors were:

\begin{tabular}{|c|c|}
  Feature & Beta \\
  \hline
  hockey & -2.39701629657 \\
  playoffs & -1.44564218453 \\
  golchowy & -1.17516769744 \\
  ice & -1.03574704859 \\
  next & -1.02160900872 \\
  goals & -1.01665832771 \\
  pick & -1.01647140833 \\
  playoff & -0.976664795926 \\
  points & -0.954740265196 \\
  biggest & -0.934753162921 \\
  
\end{tabular}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
